import json
import os
from services import bedrock_agent_runtime
import streamlit as st
import uuid
import re

# Get config from environment variables
agent_id = st.secrets["AWS_AGENT_ID"]
agent_alias_id = st.secrets["AWS_ALIAS_ID"]
ui_title = "ScoutBot"
ui_icon = ""

def init_state():
    st.session_state.session_id = str(uuid.uuid4())
    st.session_state.messages = []
    st.session_state.citations = []
    st.session_state.trace = {}

# General page configuration and initialization
st.set_page_config(page_title=ui_title, page_icon=ui_icon, layout="wide")
st.title(ui_title)
if len(st.session_state.items()) == 0:
    init_state()

# Sidebar button to reset session state
with st.sidebar:
    if st.button("Reset Session"):
        init_state()

# Messages in the conversation
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"], unsafe_allow_html=True)

# Chat input that invokes the agent
if prompt := st.chat_input():
    # Check Prompt for team building, provide additional context
    if re.search(r"\bbuild a team\b", prompt.lower()):
        modified_prompt = (
            prompt 
            + " List exactly 5 players with their respective roles. Each player must be a separate entry, even if their roles are the same."
            + " Format as follows: (IGL)1. PlayerName - Role, 2. PlayerName - Role, 3. PlayerName - Role, 4. PlayerName - Role, 5. PlayerName - Role."
            + " Ensure there are no more than 5 players."
            + " Answer questions about player performance with specific agents (in-game playable characters)."
            + " Include category of agent."
            + " Provide team strategy, strengths, and weaknesses."
        )
    else:
        modified_prompt = prompt

    st.session_state.messages.append({"role": "user", "content": modified_prompt})
    with st.chat_message("user"):
        st.write(prompt)

    with st.chat_message("assistant"):
        placeholder = st.empty()
        placeholder.markdown("thinking...")
        response = bedrock_agent_runtime.invoke_agent(
            agent_id,
            agent_alias_id,
            st.session_state.session_id,
            prompt
        )
        output_text = response["output_text"]

        # Add citations
        if len(response["citations"]) > 0:
            citation_num = 1
            num_citation_chars = 0
            citation_locs = ""
            for citation in response["citations"]:
                end_span = citation["generatedResponsePart"]["textResponsePart"]["span"]["end"] + 1
                for retrieved_ref in citation["retrievedReferences"]:
                    citation_marker = f"[{citation_num}]"
                    output_text = output_text[:end_span + num_citation_chars] + citation_marker + output_text[end_span + num_citation_chars:]
                    citation_locs = citation_locs + "\n<br>" + citation_marker + " " + retrieved_ref["location"]["s3Location"]["uri"]
                    citation_num = citation_num + 1
                    num_citation_chars = num_citation_chars + len(citation_marker)
                output_text = output_text[:end_span + num_citation_chars] + "\n" + output_text[end_span + num_citation_chars:]
                num_citation_chars = num_citation_chars + 1
            output_text = output_text + "\n" + citation_locs

        placeholder.markdown(output_text, unsafe_allow_html=True)
        st.session_state.messages.append({"role": "assistant", "content": output_text})
        st.session_state.citations = response["citations"]
        st.session_state.trace = response["trace"]

# trace_types_map = {
#     "Pre-Processing": ["preGuardrailTrace", "preProcessingTrace"],
#     "Orchestration": ["orchestrationTrace"],
#     "Post-Processing": ["postProcessingTrace", "postGuardrailTrace"]
# }

# trace_info_types_map = {
#     "preProcessingTrace": ["modelInvocationInput", "modelInvocationOutput"],
#     "orchestrationTrace": ["invocationInput", "modelInvocationInput", "modelInvocationOutput", "observation", "rationale"],
#     "postProcessingTrace": ["modelInvocationInput", "modelInvocationOutput", "observation"]
# }

# Sidebar section for trace
# with st.sidebar:
#     st.title("Trace")

#     # Show each trace types in separate sections
#     step_num = 1
#     for trace_type_header in trace_types_map:
#         st.subheader(trace_type_header)

#         # Organize traces by step similar to how it is shown in the Bedrock console
#         has_trace = False
#         for trace_type in trace_types_map[trace_type_header]:
#             if trace_type in st.session_state.trace:
#                 has_trace = True
#                 trace_steps = {}

#                 for trace in st.session_state.trace[trace_type]:
#                     # Each trace type and step may have different information for the end-to-end flow
#                     if trace_type in trace_info_types_map:
#                         trace_info_types = trace_info_types_map[trace_type]
#                         for trace_info_type in trace_info_types:
#                             if trace_info_type in trace:
#                                 trace_id = trace[trace_info_type]["traceId"]
#                                 if trace_id not in trace_steps:
#                                     trace_steps[trace_id] = [trace]
#                                 else:
#                                     trace_steps[trace_id].append(trace)
#                                 break
#                     else:
#                         trace_id = trace["traceId"]
#                         trace_steps[trace_id] = [
#                             {
#                                 trace_type: trace
#                             }
#                         ]

#                 # Show trace steps in JSON similar to the Bedrock console
#                 for trace_id in trace_steps.keys():
#                     with st.expander(f"Trace Step " + str(step_num), expanded=False):
#                         for trace in trace_steps[trace_id]:
#                             trace_str = json.dumps(trace, indent=2)
#                             st.code(trace_str, language="json", line_numbers=trace_str.count("\n"))
#                     step_num = step_num + 1
#         if not has_trace:
#             st.text("None")

#     st.subheader("Citations")
#     if len(st.session_state.citations) > 0:
#         citation_num = 1
#         for citation in st.session_state.citations:
#             for retrieved_ref_num, retrieved_ref in enumerate(citation["retrievedReferences"]):
#                 with st.expander("Citation [" + str(citation_num) + "]", expanded=False):
#                     citation_str = json.dumps({
#                         "generatedResponsePart": citation["generatedResponsePart"],
#                         "retrievedReference": citation["retrievedReferences"][retrieved_ref_num]
#                     }, indent=2)
#                     st.code(citation_str, language="json", line_numbers=trace_str.count("\n"))
#                 citation_num = citation_num + 1
#     else:
#         st.text("None")
